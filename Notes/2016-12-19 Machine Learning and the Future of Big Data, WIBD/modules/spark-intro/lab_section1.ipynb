{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Section 1\n",
    "\n",
    "\n",
    "-----\n",
    "Part 1: RDD and Spark Basics\n",
    "-----\n",
    "\n",
    "Let's get familiar with the basics of Spark (PySpark). \n",
    "\n",
    "## 1) Spark Contexts\n",
    "\n",
    "Check if you have a `SparkContext`. If not, initiate a `SparkContext`. A `SparkContext` specifies where your cluster is, i.e. the resources for all your distribute computation. Specify your `SparkContext` as follows:\n",
    "   \n",
    "```python\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "```\n",
    "\n",
    "## 2) Making your RDD\n",
    "\n",
    "Spark keeps your data in __Resilient Distributed Datasets (RDDs)__. An RDD is a collection of data partitioned across machines. Each group of records that is processed by a single thread (*task*) on a particular machine on a single machine is called a *partition*. Using RDDs Spark can process your data in parallel across the cluster. \n",
    "   \n",
    "You can create an RDD from a list, from a file or from an existing RDD.\n",
    "   \n",
    "Let's create an RDD from a Python list:\n",
    "   \n",
    "```python\n",
    "list_rdd = sc.parallelize([419, 789, 57, 83, 805, 898, 419, 260, 83, 872])\n",
    "```\n",
    "\n",
    "\n",
    "## 3) Designing a Transformation Plan\n",
    "\n",
    "RDDs are lazy so they don't perform operations unless it is needed. Transformations create a plan to change the data. \n",
    "\n",
    " ```python\n",
    "list_rdd.filter(lambda x: x>800) # Find all values greater than 800\n",
    " ```\n",
    "\n",
    "Note that this action produces no results. It is simply a plan to manipulate the data. Each RDD knows what it has to do when it is asked to produce data. Actions then instantiate the plan to produce a value.\n",
    "\n",
    "When you use `take()` or `first()` to inspect an RDD does it load the entire file or just the partitions it needs to produce the results. It is smart and lazy, just loads the partitions it needs.\n",
    " \n",
    " ```python\n",
    " list_rdd.first() # Views the first entry\n",
    " list_rdd.take(2) # Views the first two entries\n",
    "\n",
    "```\n",
    "\n",
    "## 4) Collection results\n",
    "\n",
    "If you want to get all the data from the partitions to be sent back to the driver you can do that using `collect()`. However, if your dataset is large this will __kill__ â˜  the driver. Only do this when you are developing with a small test dataset.\n",
    "   \n",
    "```python\n",
    "list_rdd.collect()\n",
    "```\n",
    "\n",
    "----\n",
    "Part 2: Explore Apple stock prices\n",
    "----\n",
    "\n",
    "![](http://apple-stock-news.com/wp-content/uploads/2016/01/appppp.jpg)\n",
    "\n",
    "Normally we would download the Apple stock prices. This would be done using the protocol below:\n",
    "\n",
    "Either open the link in Chrome or download with Python:\n",
    "\n",
    "```python\n",
    "import urllib2\n",
    "url = 'http://real-chart.finance.yahoo.com/table.csv?s=AAPL&g=d&ignore=.csv'\n",
    "csv = urllib2.urlopen(url).read()\n",
    "with open('aapl.csv','w') as f: f.write(csv)        \n",
    "```\n",
    "\n",
    "This has fortunately been done for you, and waits ready for you in the `/data` folder included in this directory, as `aapl.csv`.\n",
    "\n",
    "\n",
    "### (Group) Exercises\n",
    "\n",
    "Using Spark, answer the below questions in your small groups (you can use shell scripting to check your answers in many cases):\n",
    "\n",
    "A) How many records are there in this CSV?\n",
    "\n",
    "B) Find the average *adjusted close* price of the stock. Also find the min, max, variance, and standard deviation.\n",
    "\n",
    "C) Find the dates of the 3 highest adjusted close prices.\n",
    "\n",
    "D) Find the date of the 3 lowest adjusted close prices.\n",
    "\n",
    "E) Find the number of days on which the stock price fell, i.e. the close price was lower than the open.\n",
    "\n",
    "F) Find the number of days on which the stock price rose.\n",
    "\n",
    "G) Find the number of days on which the stock price neither fell nor rose.\n",
    "\n",
    "H) To find out how much the stock price changed on a particular day, convert the close and the open prices to natural log values using `math.log()` and then take the difference between the close and the open. This gives you the log change in the price. Find the 3 days on which the price increased the most.\n",
    "\n",
    "I) The log change price lets you calculate the average change by taking the average of the log changes. Calculate the average change in log price over the entire range of prices.\n",
    "\n",
    "---\n",
    "Hints\n",
    "----\n",
    "\n",
    "[Python RDD documentation](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
