{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Initiating a `SparkSession`\n",
    "\n",
    "1\\. Initiate a `SparkSession`. A `SparkSession` initializes both a `SparkContext` and a `SQLContext` to use RDD-based and DataFrame-based functionalities of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to SparkSQL\n",
    "\n",
    "SparkSQL allows you to execute relational queries on **structured** data using \n",
    "Spark. Today we'll get some practice with this by running some queries on a \n",
    "Yelp dataset. To begin, you will load data into a Spark `DataFrame`, which can \n",
    "then be queried as a SQL table. \n",
    "\n",
    "1\\. Load the Yelp business data using the function `.read.json()` from the `SparkSession()` object, with input file `data/yelp_academic_dataset_business.json.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Print the schema and register the `yelp_business_df` as a temporary \n",
    "table named `yelp_business` (this will enable us to query the table later using \n",
    "our `SparkSession()` object).\n",
    "\n",
    "Now, you can run SQL queries on the `yelp_business` table. For example:\n",
    "\n",
    "```python\n",
    "result = spark.sql(\"SELECT name, city, state, stars FROM yelp_business LIMIT 10\")\n",
    "result.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Write a query or a sequence of transformations that returns the `name` of entries that fulfill the following \n",
    "conditions:\n",
    "\n",
    "   - Rated at 5 `stars`\n",
    "   - In the `city` of Phoenix\n",
    "   - Accepts credit card (Reference the `'Accepts Credit Card'` field by \n",
    "   ``` attributes.`Accepts Credit Cards` ```)\n",
    "   - Contains Restaurants in the `categories` array.  \n",
    "\n",
    "   Hint: `LATERAL VIEW explode()` can be used to access the individual elements\n",
    "   of an array (i.e. the `categories` array). For reference, you can see the \n",
    "   [first example](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView) on this page.\n",
    "   \n",
    "   Hint: In spark, while using `filter()` or `where()`, you can create a condition that tests if a column, made of an array, contains a given value. The functions is [pyspark.sql.functions.array_contains](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.array_contains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Spark and SparkSQL in Practice \n",
    "\n",
    "Now that we have a basic knowledge of how SparkSQL works, let's try dealing with a real-life scenario where some data manipulation/cleaning is required before we can query the data with SparkSQL. We will be using a dataset of user information and a data set of purchases that our users have made. We'll be cleaning the data in a regular Spark RDD before querying it with SparkSQL.\n",
    "\n",
    "   1\\. Load a dataframe `users` from S3 link `''s3a://sparkdatasets/users.txt'` (no credentials needed) using `spark.read.csv` with the following parameters: no headers, use separator `\";\"`, and infer the schema of the underlying data (for now). Use `.show(5)` and `.printSchema()` to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2\\. Create a schema for this dataset using proper names and types for the columns, using types from the `pyspark.sql.types` module (see lecture). Use that schema to read the `users` dataframe again and use `.printSchema()` to check the result.\n",
    "   \n",
    "   Note: Each row in the `users` file represents the user with his/her `user_id, name, email, phone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   3\\. Load an RDD `transactions_rdd` from S3 link `''s3a://sparkdatasets/transactions.txt'` (no credentials needed) using `spark.sparkContext.textFile`. Use `.take(5)` to check the result.\n",
    "   \n",
    "   Use `.map()` to split those csv-like lines, to strip the dollar sign on the second column, and to cast each column to its proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4\\. Create a schema for this dataset using proper names and types for the columns, using types from the `pyspark.sql.types` module (see lecture). Use that schema to convert `transactions_rdd` into a dataframe `transactions`  and use `.show(5)` and `.printSchema()` to check the result.\n",
    "   \n",
    "   Each row in the `transactions` file has the columns  `user_id, amount_paid, date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Write a sequence of transformations or a SQL query that returns the names and the amount paid for the users with the **top 10** transaction amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
