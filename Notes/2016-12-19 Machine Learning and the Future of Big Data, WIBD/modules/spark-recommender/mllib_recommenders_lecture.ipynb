{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://kodcu.com/wp/wp-content/uploads/2014/06/mllib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://img.scoop.it/_VP0qLV-jYbp2cFF4H4k4jl72eJkfbmt4t8yenImKBVvK0kTmF0xjctABnaLJIm9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/b7FNjKdGXEFos/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MLlib Overview\n",
    "---\n",
    "\n",
    "MLlib is a Spark subproject (originally started in AMPlab in Berkeley) providing machine learning primitives that are \"production\" ready. This is an incredible benefit to the data science community.\n",
    "\n",
    "Widespread adoption and support - 80+ contributors from various organizations. \n",
    " \n",
    "__Remember__: You do not _need_ to use MLlib to do large scale machine learning. \n",
    "\n",
    "Depending on the nature of the problem there are other ways to do large scale machine learning:\n",
    "\n",
    "1. Use a large single-node machine learning library (_i.e._ `scikit-learn`) on big EC2 instances.\n",
    "2. Distribute embarassingly parallel problems as a grid search  (relatively common). \n",
    "3. Use MapReduce to design a model composed of (several) multiple map-reduce steps. This is essentially what MLLib does for you, in so many words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "[MLlib Algorithms](http://spark.apache.org/mllib/)\n",
    "----\n",
    "- logistic regression and linear support vector machine (SVM)\n",
    "- classification and regression tree\n",
    "- random forest and gradient-boosted trees\n",
    "- recommendation via alternating least squares (ALS)\n",
    "- clustering via k-means, bisecting k-means, Gaussian mixtures (GMM), and power iteration clustering\n",
    "- topic modeling via latent Dirichlet allocation (LDA)\n",
    "- survival analysis via accelerated failure time model\n",
    "- singular value decomposition (SVD) and QR decomposition\n",
    "- principal component analysis (PCA)\n",
    "- linear regression with L1, L2, and elastic-net regularization\n",
    "- isotonic regression\n",
    "- multinomial/binomial naive Bayes\n",
    "- frequent itemset mining via FP-growth and association rules\n",
    "- sequential pattern mining via PrefixSpan\n",
    "- summary statistics and hypothesis testing\n",
    "- feature transformations\n",
    "- model evaluation and hyper-parameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As wondrous as the implementation of these models are, their very presence will not save you from a bad choice of model, poorly-manicured data (bad pipelines/feature engineering), or a lack of signal. You still have to be a data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Points to Ponder\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What types of algorithms are not in Spark? Why do you think that is?\n",
    "</summary>\n",
    "Deep Learning. [They are working on it](http://arxiv.org/abs/1511.06051)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Spark Statistics\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x11531f6d0>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "# Creating SparkContext not necessary,\n",
    "# because of launching notebook with pyspark \n",
    "# `$ IPYTHON_OPTS=\"notebook\" pyspark` \n",
    "sc = pyspark.SparkContext()\n",
    "# sc = pyspark.SparkContext() \n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-12e05460d4e2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-12e05460d4e2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Statistics.\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the what is available...\n",
    "Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   1.,   10.,  100.]),\n",
       " array([   2.,   20.,  200.]),\n",
       " array([   3.,   30.,  300.])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RDD of Vectors\n",
    "mat = sc.parallelize([np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])])  \n",
    "mat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute column summary statistics.\n",
    "summary = Statistics.colStats(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-345574200ac9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-345574200ac9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    summary.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the what is available...\n",
    "summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of each column: [   2.   20.  200.]\n",
      "The mean of each clumn: [  1.00000000e+00   1.00000000e+02   1.00000000e+04]\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean of each column: {}\".format(summary.mean()))\n",
    "print(\"The mean of each column: {}\".format(summary.variance()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<p><details><summary>What common statistics are missing from this list? Why?</summary>\n",
    "Median and other quantiles are missing.  \n",
    "<br>\n",
    "Why? Because they are non-associative, thus harder to parrelleize\n",
    "</details></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# k-means Algorithm\n",
    "\n",
    "The k-means algorithm\n",
    "\n",
    "* Choose a number of clusters k\n",
    "* Randomly assign each point to a cluster\n",
    "* Repeat:\n",
    "    * a\\. For each of k clusters, compute cluster *centroid* by taking\n",
    "mean vector of points in the cluster\n",
    "    * b\\. Assign each data point to cluster for which centroid is closest\n",
    "(Euclidean)\n",
    "\n",
    "...until clusters stop changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/mllib-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/choose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/adjust.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "K-means in Spark 2.0\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n",
    "         (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n",
    "df = sqlContext.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=2, seed=1)\n",
    "model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source]( https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.ml.html?highlight=kmeans#pyspark.ml.clustering.KMeans )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Points to Ponder\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "1) Why is K-means in Spark's MLlib? A better reason to say this is: Why bother to put K-means in MLLib in the first place?\n",
    "</summary>\n",
    "K-means requires a complete pass over the data every iteration. Spark is  efficient for that specification and so makes the implementation of the algorithm worthwhile. In general, calculations that are of quadratic order\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "Recommendations via matrix completion\n",
    "-----\n",
    "\n",
    "![](images/recommend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering\n",
    "---\n",
    "\n",
    "Goal: predict users’ movie ratings based on past ratings of other movies\n",
    "\n",
    "$$\n",
    "R = \\left( \\begin{array}{ccccccc}\n",
    "1 & ? & ? & 4 & 5 & ? & 3 \\\\\n",
    "? & ? & 3 & 5 & ? & ? & 3 \\\\\n",
    "5 & ? & 5 & ? & ? & ? & 1 \\\\\n",
    "4 & ? & ? & ? & ? & 2 & ?\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details><summary>\n",
    "What are the challenges of large scale Collaborative Filtering?\n",
    "</summary>\n",
    "Challenges: <br>\n",
    "- Defining similarity that scales <br>\n",
    "- Dimensionality (Millions of Users / Items) <br>\n",
    "- Sparsity, most users has not seen most items <br>\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Why Spark for collaborative filtering? Why not Map-Reduce?\n",
    "</summary>\n",
    "a) In-memory processing of the data  \n",
    "b) Real-time data processing capabilities\n",
    "c) Easier implementation\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "Every matrix has a *unique* decomposition in the following form:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![SVD](images/SVD.png)\n",
    "\n",
    "where\n",
    "* *U* is column orthogonal: *U<sup>T</sup>U = I*\n",
    "* *V* is column orthogonal: *V<sup>T</sup>V = I*\n",
    "* *Sigma* is a diagonal matrix of positive values, where the diagonal is ordered in decreasing order\n",
    "\n",
    "We can reduce the dimensions by sending the smaller of the diagonals to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD for topic analysis\n",
    "\n",
    "We can use SVD to determine what we call ***latent features***. This will be best demonstrated with an example.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie.\n",
    "\n",
    "|           | Matrix | Alien | Serenity | Casablanca | Amelie |\n",
    "| --------- | ------ | ----- | -------- | ---------- | ------ |\n",
    "| **Alice** |      1 |     1 |        1 |          0 |      0 |\n",
    "|   **Bob** |      3 |     3 |        3 |          0 |      0 |\n",
    "| **Cindy** |      4 |     4 |        4 |          0 |      0 |\n",
    "|   **Dan** |      5 |     5 |        5 |          0 |      0 |\n",
    "| **Emily** |      0 |     2 |        0 |          4 |      4 |\n",
    "| **Frank** |      0 |     0 |        0 |          5 |      5 |\n",
    "|  **Greg** |      0 |     1 |        0 |          2 |      2 |\n",
    "\n",
    "Note that the first three movies (Matrix, Alien, Serenity) are Sci-fi movies and the last two (Casablanca, Amelie) are Romance. We will be able to mathematically pull out these topics!\n",
    "\n",
    "Let's do the computation with Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0]\n",
      " [3 3 3 0 0]\n",
      " [4 4 4 0 0]\n",
      " [5 5 5 0 0]\n",
      " [0 2 0 4 4]\n",
      " [0 0 0 5 5]\n",
      " [0 1 0 2 2]]\n",
      "=\n",
      "[[-0.14  0.02  0.01  0.99 -0.   -0.    0.  ]\n",
      " [-0.41  0.07  0.03 -0.06 -0.89  0.19  0.  ]\n",
      " [-0.55  0.09  0.04 -0.08  0.42  0.71  0.  ]\n",
      " [-0.69  0.12  0.05 -0.1   0.19 -0.68  0.  ]\n",
      " [-0.15 -0.59 -0.65 -0.    0.   -0.   -0.45]\n",
      " [-0.07 -0.73  0.68  0.   -0.    0.    0.  ]\n",
      " [-0.08 -0.3  -0.33 -0.   -0.   -0.    0.89]]\n",
      "[ 12.48   9.51   1.35   0.     0.  ]\n",
      "[[-0.56 -0.59 -0.56 -0.09 -0.09]\n",
      " [ 0.13 -0.03  0.13 -0.7  -0.7 ]\n",
      " [ 0.41 -0.8   0.41  0.09  0.09]\n",
      " [-0.71  0.    0.71 -0.    0.  ]\n",
      " [-0.    0.   -0.    0.71 -0.71]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.linalg import svd\n",
    "\n",
    "M = np.array([[1, 1, 1, 0, 0],\n",
    "              [3, 3, 3, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [5, 5, 5, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 1, 0, 2, 2]])\n",
    "\n",
    "u, e, v = svd(M)\n",
    "print M\n",
    "print \"=\"\n",
    "print np.around(u, 2)\n",
    "print np.around(e, 2)\n",
    "print np.around(v, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's the results we get:\n",
    "\n",
    "![ ](images/svd_example.png)\n",
    "\n",
    "Note that the last two singular values are 0, so we can drop them. Note that these values are 0 because the rank of our original matrix is 3.\n",
    "\n",
    "![SVD Example](images/svd_example2.png)\n",
    "\n",
    "You can see the two topics fall out:\n",
    "\n",
    "1. Science Fiction\n",
    "    * First singular value (12.4)\n",
    "    * First column of the *U* matrix (note that the first four users have large values here)\n",
    "    * First row of the *V* matrix (note that the first three movies have large values here)\n",
    "2. Romance\n",
    "    * Second singular value (9.5)\n",
    "    * Second column of the *U* matrix (note that the last three users have large values here)\n",
    "    * Second row of the *V* matrix (note that the last two movies have large values here)\n",
    "\n",
    "*U* is the ***user-to-topic*** matrix and *V* is the ***movie-to-topic*** matrix.\n",
    "\n",
    "The third singular value is relatively small, so we can exclude it with little loss of data. Let's try doing that and reconstruct our matrix!\n",
    "\n",
    "\n",
    "![SVD Example](images/svd_example3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares  \n",
    "![ALS](https://databricks.com/wp-content/uploads/2014/07/als-illustration.png)  \n",
    "1. Start with random $U_1$, $V_1$\n",
    "2. (LU decomposition) Solve for $U_2$ to minimize $||R – U_2V_1^T||$ subject to $V_1$\n",
    "3. (LU decomposition) Solve for $V_2$ to minimize $||R – U_2V_2^T||$ subject to $U_2$\n",
    "4. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "ALS in Spark 2.0\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-18-7d5d428850d2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-7d5d428850d2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    [\"user\", \"item\", \"rating\"])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# df = sqlContext.createDataFrame([(0, 1, 4.0), (1, 1, 5.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n",
    "                           [\"user\", \"item\", \"rating\"])\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# als = ALS() \n",
    "# model = als.train(df, rank=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# user = 0\n",
    "# item = 2\n",
    "# model.predict(user, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ALS vs. SVD for recommendation engines\n",
    "---\n",
    "\n",
    "Alternating least squares (ALS) is flexible but less precise.  \"Approximate\" means minimizing some squared-error difference with the input A, but, here you can customize exactly what is considered in the loss function. For example you can ignore missing values (crucial) or weight different values differently.\n",
    "\n",
    "Singular value decomposition (SVD) is a decomposition that gives more guarantees about its factorization. The SVD is relatively more computationally expensive and harder to parallelize. There is also not a good way to deal with missing values or weighting; you need to assume that in your sparse input, missing values are equal to a mean value 0. \n",
    "\n",
    "[Source](https://www.quora.com/What-is-the-difference-between-SVD-and-matrix-factorization-in-context-of-recommendation-engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "- MLlib is Spark's machine learning framework. \n",
    "- A bunch of super smart people are porting even more algorithms to work in distributed, lazy-exectution way.+987\n",
    "- MLlib has the greatest hits of machine learning:\n",
    "    - K-means for clustering\n",
    "    - ALS for Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bonus Materials\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MLlib Data Types\n",
    "---\n",
    "\n",
    "[Docs](http://spark.apache.org/docs/latest/mllib-data-types.html)\n",
    "\n",
    "`Local vector`: can be dense or sparse\n",
    "\n",
    "A dense vector is backed by a double array representing its entry values.\n",
    "\n",
    "While a sparse vector is backed by two parallel arrays: indices and values. \n",
    "\n",
    "For example, a vector (1.0, 0.0, 3.0) can be represented in \n",
    "\n",
    "| Dense | Sparse |  \n",
    "|:-------:|:------:|\n",
    "| [1.0, 0.0, 3.0]  | (3, [0, 2], [1.0, 3.0]) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 2: 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint`\n",
    "\n",
    "A labeled point is a local vector, either dense or sparse, associated with a label/response for supervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local matrix\n",
    "\n",
    "Take a _wild guess_...\n",
    "\n",
    "> A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed matrix\n",
    "\n",
    "> A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. \n",
    "\n",
    "It is very important to choose the right format to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Three types of distributed matrices have been implemented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-ff1cbc6e9998>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-ff1cbc6e9998>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    rowsRDD.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rowsRDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<details><summary>\n",
    "Which data type would you use for random forests?\n",
    "</summary>\n",
    "`LabeledPoint` <br>\n",
    "<br>\n",
    "Random forests is a supervised learning algorithm. It requires labels in order to classify.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
